================================================================================
DEVOP-SIMPLE-PROJECT - COMPREHENSIVE APPLICATION DOCUMENTATION
================================================================================

Project Date: February 11, 2026
Purpose: Complete microservice application with Kubernetes deployment, CI/CD, and monitoring
Owner: fusssspring

================================================================================
1. PROJECT OVERVIEW
================================================================================

DevOp-Simple-Project is a cloud-native microservice application designed to demonstrate
modern DevOps practices. It consists of:

  - Backend Service: FastAPI-based REST API with Prometheus metrics
  - Frontend Service: Nginx reverse proxy and static web server
  - Monitoring Stack: Prometheus for metrics collection, Grafana for visualization
  - CI/CD Pipeline: Jenkins-based automated build and deployment
  - Infrastructure-as-Code: Terraform for Kubernetes resource management
  - Deployment Automation: Ansible for orchestrating builds, pushes, and deployments
  - Container Orchestration: Docker and Kubernetes for deployment

The application is designed to run on any Kubernetes cluster (Minikube, EKS, AKS, GKE, etc.)
and supports both local development via docker-compose and production deployment via Kubernetes.

================================================================================
2. DIRECTORY STRUCTURE & FILE ROLES
================================================================================

ROOT DIRECTORY:
  - docker-compose.yml        : Local development orchestration (backend, frontend)
  - Jenkinsfile               : CI/CD pipeline definition (8 stages: Checkout → Deploy)
  - main.tf                   : Root Terraform configuration (calls terraform/ module)
  - .terraform.lock.hcl       : Terraform provider lock file
  - terraform.tfstate         : Terraform state file (tracks infrastructure)
  - terraform.tfstate.backup  : Backup of Terraform state
  - MONITORING.md             : Guide for Prometheus & Grafana setup (271 lines)
  - TERRAFORM.md              : Terraform configuration documentation
  - README.md                 : Project overview and quick start
  - metrics-deploy-fixed.yaml : Kubernetes manifest for metrics deployment (legacy)

BACKEND/ (Python FastAPI application):
  - backend.py                : Main FastAPI application (56 lines)
                                * Defines /health and /version endpoints
                                * Includes Prometheus metrics collection middleware
                                * Tracks HTTP requests, status codes, and duration
  - Dockerfile                : Container image for backend
                                * Base: Python 3.11
                                * Runs uvicorn server on port 8000
  - requirements.txt          : Python dependencies:
                                * fastapi==0.104.1
                                * uvicorn[standard]==0.24.0
                                * prometheus-client==0.19.0
                                * python-dotenv==1.0.0

FRONTEND/ (Nginx web server & static files):
  - frontend.html             : Static HTML served to clients
  - nginx.conf                : Nginx configuration (65 lines)
                                * Upstream backend service on port 8000
                                * Reverse proxy for /api/* to backend
                                * Static file serving on root path
                                * Client max body size: 20MB
  - Dockerfile                : Container image for frontend
                                * Base: Nginx Alpine
                                * Copies nginx.conf and frontend.html

TERRAFORM/ (Infrastructure-as-Code for Kubernetes):
  All files use the Kubernetes provider to declaratively manage k8s resources.
  
  - main.tf                   : Module entry point and Terraform version constraint (>=1.0.0)
  - provider.tf               : Kubernetes provider configuration
                                * Specifies kubeconfig path and context (default: minikube)
                                * Requires Kubernetes provider ~> 2.0
  - variables.tf              : Input variables (103 lines)
                                * kubeconfig_path, kubeconfig_context
                                * namespace (default: "default")
                                * backend/frontend image and port configuration
                                * prometheus/grafana image configuration
                                * hpa settings (min/max replicas, cpu threshold)
                                * ingress hostname
                                * grafana admin password
  - locals.tf                 : Local values and namespace resource
                                * Creates kubernetes_namespace_v1.default
  - deployments.tf            : Kubernetes Deployment definitions (286 lines)
                                * Backend deployment (configurable replicas)
                                * Frontend deployment (1 replica)
                                * Prometheus deployment with config volume mount
                                * Grafana deployment with provisioning volume mount
                                * All include resource limits, health checks, annotations
  - services.tf               : Kubernetes Service definitions (190 lines)
                                * Backend ClusterIP service on port 8000
                                * Frontend ClusterIP service on port 80
                                * Prometheus ClusterIP service on port 9090
                                * Grafana ClusterIP service on port 3000
                                * All services use selector labels for pod discovery
  - configmaps.tf             : Kubernetes ConfigMap resources
                                * prometheus-config: Loads prometheus.yml from k8s/ directory
                                * prometheus-rules: Loads prometheus.rules from k8s/ directory
                                * grafana-provisioning: Loads grafana datasource/dashboard configs
  - hpa.tf                    : Horizontal Pod Autoscaler (84 lines)
                                * Targets backend deployment
                                * Conditional creation (enable_hpa variable)
                                * CPU-based scaling (default: 70% threshold)
                                * Min replicas: configurable (default: 1), Max: configurable (default: 5)
  - rbac.tf                   : Role-Based Access Control (66 lines)
                                * ServiceAccount for Prometheus
                                * ClusterRole with permissions to access nodes, services, endpoints
                                * ClusterRoleBinding to grant permissions
  - outputs.tf                : Terraform output values (55 lines)
                                * Service names and endpoints (DNS format)
                                * Deployment names
                                * HPA status and replica info
                                * Exposed for downstream tool consumption

K8S/ (Static Kubernetes configuration):
  Used as reference but also loaded into ConfigMaps via Terraform.
  
  - deployments.yaml          : Pod, Deployment, Service definitions (legacy YAML)
  - prometheus.yml            : Prometheus scrape configuration
                                * Global scrape interval: 15 seconds
                                * Kubernetes service discovery (scrape endpoints)
  - prometheus.rules          : Prometheus alerting rules (empty in base)
  - grafana-datasources.yml   : Grafana data source provisioning
                                * Configures Prometheus as data source
  - grafana-dashboards.yml    : Grafana dashboard provisioning
                                * Pre-loads dashboards on startup

ANSIBLE/ (Deployment orchestration):
  Ansible playbook for building, pushing, and deploying the application.
  
  - playbook.yml              : Main Ansible playbook
                                * Builds backend & frontend Docker images
                                * Pushes images to Docker registry (docker.io)
                                * Applies Kubernetes manifests from k8s/ directory
                                * Patches Kubernetes Deployments to update image references
                                * Waits for rollout to complete
                                * Configurable registry, namespace, build tag via env vars
  - inventory.ini             : Ansible inventory
                                * localhost with local connection for CI/CD agent
  - requirements.yml          : Ansible Galaxy requirements
                                * community.docker collection (~>3.0)
                                * kubernetes.core collection (~>3.0)
  - README.md                 : Usage instructions for Ansible playbook

DEPLOY/ (Proposed deployment structure - see note below):
  Note: This folder structure was proposed but undone per user request.
  Currently, Ansible and Helm are in top-level ansible/ and helm/ folders.
  The proposed structure would be:
    deploy/
      ├── ansible/         (move from root ansible/)
      ├── helm/            (move from root helm/)
      └── terraform/       (reference to root terraform/)

================================================================================
3. APPLICATION COMPONENTS DETAILED BREAKDOWN
================================================================================

3.1 BACKEND SERVICE (FastAPI)
----------------------------
Location: backend/backend.py

Features:
  - FastAPI 0.104.1 framework for REST API
  - Uvicorn ASGI server listening on 0.0.0.0:8000
  - Health check endpoint: GET /health → {"status": "ok"}
  - Version endpoint: GET /version → {"version": "dev" or APP_VERSION env var}
  - Prometheus metrics middleware that instruments ALL requests:
    * http_requests_total counter: Labeled by method, endpoint, status code
    * http_request_duration_seconds histogram: Duration in seconds
  - Automatic metrics export for Prometheus scraping

Metrics Collected:
  - Every HTTP request increments http_requests_total counter
  - Every HTTP request duration recorded in histogram
  - Labels allow filtering by HTTP method, endpoint path, status code
  - /metrics endpoint (FastAPI default) exposes metrics in Prometheus format

Deployment:
  Via Kubernetes Deployment (terraform/deployments.tf):
    - Image: configurable via variable.backend_image
    - Replicas: configurable, default 2
    - Port: 8000 (configurable)
    - Environment: APP_VERSION set from secret/config
    - Health check: HTTP GET /health
    - Resource limits: CPU and memory configurable
    - Prometheus scrape annotations: Tells Prometheus to scrape this pod

3.2 FRONTEND SERVICE (Nginx)
----------------------------
Location: frontend/

nginx.conf Configuration (65 lines):
  - Runs as nginx user
  - Worker processes: auto (matches CPU cores)
  - Access & error logging enabled
  - Upstream backend: Points to backend:8000 service DNS
  - HTTP server on port 80 (configurable)
  - Root location (/): Serves static files from /usr/share/nginx/html
  - /api/* location: Reverse proxy to backend service
  - WebSocket upgrade headers: Enabled for real-time connections
  - Client max body: 20MB
  - Health check endpoint: /nginx-health (configured in docker-compose.yml)

frontend.html:
  - Static HTML file served to clients
  - Likely contains JavaScript for interacting with backend API
  - Located in /usr/share/nginx/html/ (set by Dockerfile)

Deployment:
  Via Kubernetes Deployment (terraform/deployments.tf):
    - Image: configurable via variable.frontend_image
    - Replicas: 1 (fixed, no scaling)
    - Port: 80 (configurable)
    - Resource limits: configurable
    - Depends on backend service via upstream DNS

3.3 MONITORING STACK
-------------------

Prometheus (Port 9090):
  - Timeseries database for metrics
  - Scrapes HTTP endpoints every 15 seconds
  - Service discovery: Kubernetes endpoints scraping
  - Configuration: Loaded from k8s/prometheus.yml via ConfigMap
  - Stores metrics data locally on disk
  - Provides API for querying metrics

Grafana (Port 3000):
  - Visualization and dashboarding platform
  - Datasource: Prometheus (provisioned automatically)
  - Default credentials: admin / admin (changeable)
  - Pre-provisioned dashboards from k8s/grafana-dashboards.yml
  - Supports custom queries using PromQL
  - Web UI for interactive dashboard creation

Both are deployed as Kubernetes Deployments with dedicated services.

3.4 CI/CD PIPELINE (Jenkins)
---------------------------
Location: Jenkinsfile

Pipeline Structure (8 stages, 30-minute timeout):

  Stage 1: Checkout
    - Clones repository from SCM (git)
    - Fallback: uses local workspace if SCM not available

  Stage 2: Build Backend
    - Executes: docker build -t ${DOCKER_USER}/${APP_NAME}-backend:${BUILD_TAG}
    - Tags as "latest" as well
    - Location: /devop-simple-project/backend

  Stage 3: Build Frontend
    - Executes: docker build -t ${DOCKER_USER}/${APP_NAME}-frontend:${BUILD_TAG}
    - Tags as "latest" as well
    - Location: /devop-simple-project/frontend

  Stage 4: Push to Registry
    - Authenticates to Docker Hub using 'docker-creds' Jenkins credential
    - Registry: docker.io
    - Username: fusssspring (configurable in env block)
    - Pushes both backend and frontend, both ${BUILD_TAG} and latest
    - Logs out after push

  Stage 5: Test Backend
    - Placeholder test stage (currently just echoes success message)
    - Can be extended with unit tests, integration tests, etc.

  Stage 6: Test Frontend
    - Placeholder test stage
    - Can be extended with E2E tests, visual regression tests

  Stage 7: Deploy
    - Installs Ansible collections from ansible/requirements.yml
    - Exports registry credentials as env vars
    - Runs: ansible-playbook -i ansible/inventory.ini ansible/playbook.yml
    - Passes build tag and registry info as extra variables

  Stage 8: Health Check
    - Runs basic health checks (placeholder)
    - Can be extended with endpoint checks, pod readiness verification

Environment Variables (set at pipeline level):
  - APP_NAME = 'devop-simple-project'
  - DOCKER_REGISTRY = 'docker.io'
  - DOCKER_USER = 'fusssspring'
  - BUILD_TAG = "${BUILD_NUMBER}" (increments on each run)

Post-Build Actions:
  - Always: Prints completion message
  - Success: Prints service URLs (Frontend, Backend, API)
  - Failure: Prints failure message
  - Unstable: Prints unstable warning

Build Retention:
  - Keeps last 10 builds
  - Adds timestamps to logs

3.5 INFRASTRUCTURE-AS-CODE (Terraform)
-------------------------------------

Terraform manages the entire Kubernetes deployment declaratively.

Kubernetes Provider:
  - Version: ~> 2.0
  - Target: Any Kubernetes cluster
  - Kubeconfig: Specify via KUBECONFIG env var or kubeconfig_path variable
  - Context: Default "minikube" (configurable)

Resources Created:
  1. Namespace (kubernetes_namespace_v1.default)
     - Created for resource isolation
     - Labels ignored on update (lifecycle.ignore_changes)

  2. Deployments (kubernetes_deployment_v1)
     - backend: Configurable replicas, image, port
     - frontend: 1 replica (fixed for simplicity)
     - prometheus: Mounts config ConfigMap
     - grafana: Mounts provisioning ConfigMaps, sets admin password

  3. Services (kubernetes_service_v1)
     - Type: ClusterIP (internal DNS names)
     - backend, frontend, prometheus, grafana
     - Expose ports via Kubernetes DNS (e.g., backend.default.svc.cluster.local:8000)

  4. ConfigMaps (kubernetes_config_map_v1)
     - prometheus-config: prometheus.yml content
     - prometheus-rules: prometheus.rules content
     - grafana-provisioning: datasources and dashboards YAML

  5. ServiceAccount (kubernetes_service_account_v1)
     - Prometheus account for scraping permissions

  6. ClusterRole (kubernetes_cluster_role_v1)
     - Prometheus permissions to read nodes, services, endpoints, pods

  7. ClusterRoleBinding (kubernetes_cluster_role_binding_v1)
     - Binds Prometheus ServiceAccount to ClusterRole

  8. HorizontalPodAutoscaler (kubernetes_horizontal_pod_autoscaler_v2)
     - Targets backend Deployment
     - Conditional: Only created if enable_hpa = true
     - Metrics: CPU utilization (default 70% threshold)
     - Replicas: min (default 1) to max (default 5)

Workflow:
  1. User runs: terraform init (downloads Kubernetes provider)
  2. User runs: terraform plan (previews changes)
  3. User runs: terraform apply (creates/updates resources)
  4. State file: .tfstate tracks all created resources
  5. On destroy: terraform destroy (removes all resources)

3.6 DEPLOYMENT AUTOMATION (Ansible)
---------------------------------

Ansible playbook orchestrates the complete deployment workflow.

Hosts: localhost with local connection (runs on CI/CD agent machine)

Variables (configurable via environment or extra-vars):
  - registry: "docker.io"
  - registry_namespace: "fusssspring"
  - backend_name: derived from APP_NAME env var
  - frontend_name: derived from APP_NAME env var
  - build_tag: derived from BUILD_TAG env var (default "latest")
  - namespace: "default" (Kubernetes namespace)
  - kubeconfig: Read from KUBECONFIG env var
  - push_images: true (push to registry after building)

Collections Required:
  - community.docker (~> 3.0): For docker_image and docker_login modules
  - kubernetes.core (~> 3.0): For k8s module (applying manifests)

Tasks:
  1. Login to container registry (uses REGISTRY_USER/PASSWORD env vars)
  2. Build and push backend image
     - Uses community.docker.docker_image module
     - Idempotent: Only rebuilds if source changed
  3. Build and push frontend image
  4. Apply Kubernetes manifests from k8s/ directory
     - Uses kubernetes.core.k8s module
     - Applies ConfigMaps, RBAC, Deployments, Services
  5. Patch backend Deployment with new image tag
     - Strategic merge of spec.template.spec.containers[0].image
  6. Patch frontend Deployment with new image tag
  7. Wait for deployments to complete
  8. Print deployment summary

Integration with Jenkins:
  - Jenkins executes: ansible-galaxy collection install -r ansible/requirements.yml
  - Sets environment variables: REGISTRY_USER, REGISTRY_PASSWORD, BUILD_TAG
  - Executes playbook with extra-vars for registry, namespace, build tag

3.7 LOCAL DEVELOPMENT (Docker Compose)
-------------------------------------

Location: docker-compose.yml

Services:
  1. Backend Service
     - Build context: ./backend
     - Container name: devop-backend
     - Port mapping: 8000:8000
     - Environment: APP_VERSION=1.0.0
     - Network: devop-network (bridge)
     - Restart: unless-stopped
     - Health check: curl http://localhost:8000/health
       * Interval: 10s
       * Timeout: 5s
       * Retries: 3

  2. Frontend Service
     - Build context: ./frontend
     - Container name: devop-frontend
     - Port mapping: 80:80
     - Depends on: backend service (waits for health check)
     - Network: devop-network
     - Restart: unless-stopped
     - Health check: curl http://localhost/nginx-health
       * Same intervals as backend

Network:
  - Bridge driver: isolated network for service communication

Quick Start:
  docker-compose up -d      : Start all services in background
  docker-compose down       : Stop and remove all containers
  docker-compose logs -f    : Follow logs from all services
  docker-compose ps         : Show running containers

Access:
  - Frontend: http://localhost
  - Backend API: http://localhost:8000
  - Backend health: http://localhost:8000/health

================================================================================
4. DEPLOYMENT OPTIONS
================================================================================

4.1 LOCAL DEVELOPMENT
--------------------
  Setup: docker-compose up -d
  Access: http://localhost
  Backend API: http://localhost:8000
  Suitable for: Feature development, local testing
  Monitoring: No built-in (Prometheus/Grafana not included)
  Tools required: Docker, Docker Compose

4.2 KUBERNETES DEPLOYMENT (RECOMMENDED FOR PRODUCTION)
-----------------------------------------------------

  Option A: Using Terraform
    1. Configure kubeconfig: export KUBECONFIG=~/.kube/config
    2. Initialize: terraform init
    3. Plan: terraform plan
    4. Apply: terraform apply
    5. Access: Use port-forward or ingress (see MONITORING.md)
    
    Advantages:
    - Declarative, version-controllable infrastructure
    - Easy to modify variables and reapply
    - State tracking via .tfstate file
    - Reproducible deployments

  Option B: Using Ansible
    1. Install collections: ansible-galaxy collection install -r ansible/requirements.yml
    2. Configure: Set env vars (REGISTRY_USER, REGISTRY_PASSWORD, KUBECONFIG)
    3. Run: ansible-playbook -i ansible/inventory.ini ansible/playbook.yml
    
    Advantages:
    - Idempotent operations (safe to run multiple times)
    - Integrated with CI/CD pipeline (Jenkins)
    - Automatic image build, push, and deployment
    - Easy to add pre/post-deployment tasks

  Option C: Using Jenkins Pipeline
    1. Configure Jenkins: Add docker-creds credential
    2. Add Jenkinsfile to this repository
    3. Create Jenkins job pointing to this repo
    4. Trigger: Manual run or automatic on git push
    5. Monitor: Jenkins UI shows pipeline progress
    
    Advantages:
    - Fully automated CI/CD
    - Audit trail of all deployments
    - Reproducible builds
    - Easy to add approval steps, notifications

Essential Requirements for K8s:
  - kubectl configured to access target cluster
  - Kubernetes cluster (1.16+)
  - Docker (for image builds)
  - Ansible (for playbook execution)
  - Helm (optional, for easier management)

================================================================================
5. CONFIGURATION & CUSTOMIZATION
================================================================================

5.1 BACKEND CONFIGURATION
------------------------
Edit: backend/backend.py or backend/requirements.txt

Options:
  - Add new API endpoints in backend.py
  - Add Python dependencies in requirements.txt
  - Modify Prometheus metrics
  - Set APP_VERSION environment variable

5.2 FRONTEND CONFIGURATION
-------------------------
Edit: frontend/nginx.conf or frontend/frontend.html

Options:
  - Modify nginx proxy settings (backends, locations, timeouts)
  - Update HTML/CSS/JavaScript in frontend.html
  - Add additional location blocks for new routes
  - Adjust nginx worker settings

5.3 KUBERNETES CONFIGURATION
---------------------------
Edit: terraform/variables.tf or terraform/main.tf

Common customizations:
  - backend_image: your-registry/your-backend:tag
  - frontend_image: your-registry/your-frontend:tag
  - backend_replicas: 3 (for more replicas)
  - enable_hpa: true (to enable autoscaling)
  - namespace: custom-namespace

Usage:
  terraform apply -var="backend_replicas=5"
  terraform apply -var="enable_hpa=true" -var="hpa_max_replicas=10"

5.4 MONITORING CONFIGURATION
---------------------------
Edit: k8s/prometheus.yml (scrape targets), k8s/prometheus.rules (alerting)

Options:
  - Add new scrape targets in prometheus.yml (job_name sections)
  - Define alert rules in prometheus.rules
  - Import pre-built Grafana dashboards (ID 3662, 1860, etc.)
  - Create custom dashboards in Grafana UI

5.5 REGISTRY CONFIGURATION
-------------------------
For Docker Hub (default):
  - Update terraform/deployments.tf: backend_image/frontend_image variables
  - Update Jenkinsfile: DOCKER_REGISTRY, DOCKER_USER environment
  - Add Jenkins credential: docker-creds (username/password)

For private registry (ACR, ECR, GCR, etc.):
  - Update image reference: myregistry.azurecr.io/myimage:tag
  - Create pull secrets: kubectl create secret docker-registry
  - Update terraform/deployments.tf to reference pull secrets

================================================================================
6. MONITORING & OBSERVABILITY
================================================================================

6.1 PROMETHEUS METRICS
---------------------
Backend metrics exposed at: http://backend:8000/metrics (or via Prometheus default)

Exported metrics:
  - http_requests_total{method, endpoint, status}: Total request count
  - http_request_duration_seconds{method, endpoint}: Request latency histogram
  - Standard Prometheus client metrics (process, python, etc.)

Query examples in Prometheus UI (http://localhost:9090):
  - rate(http_requests_total[5m]): Requests per second (5-minute average)
  - http_request_duration_seconds_bucket: Request duration distribution
  - http_requests_total{status="500"}: Only 5xx errors

6.2 GRAFANA DASHBOARDS
---------------------
Access: http://localhost:3000
Default login: admin / admin

Built-in data source:
  - Prometheus (points to http://prometheus:9090)

Pre-configured dashboards:
  - Import "3662" (Prometheus 2.0 Stats)
  - Import "1860" (Node Exporter) if node metrics available

Custom dashboard creation:
  1. Click + > Dashboard > New Panel
  2. Select Prometheus data source
  3. Enter PromQL query (e.g., rate(http_requests_total[5m]))
  4. Customize visualization (graph, gauges, tables, heatmaps)
  5. Save dashboard

6.3 ALERTING
----------
Prometheus rules: k8s/prometheus.rules (currently empty, ready for rules)

Example alerting rules:
```
groups:
  - name: application
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        annotations:
          summary: "High error rate detected"
```

================================================================================
7. SECURITY CONSIDERATIONS
================================================================================

7.1 CURRENT SECURITY FEATURES
----------------------------
  - Registry authentication: Docker login credentials from Jenkins secrets
  - RBAC: Prometheus ServiceAccount with specific permissions
  - Network isolation: Kubernetes ClusterIP services (internal only)
  - Container running: Security context can restrict privileges

7.2 RECOMMENDED ENHANCEMENTS
---------------------------
  - TLS/HTTPS: Add Ingress with SSL certificates
  - Network policies: Restrict traffic between pods
  - Pod security policies: Enforce security standards
  - Secret management: Use Kubernetes Secrets for sensitive data
  - Image scanning: Scan images for vulnerabilities before pushing
  - RBAC refinement: App-specific ServiceAccounts instead of default
  - Audit logging: Enable Kubernetes audit logs

================================================================================
8. TROUBLESHOOTING & MAINTENANCE
================================================================================

8.1 COMMON ISSUES
----------------

Issue: Pods not running
  Solution: kubectl describe pod POD_NAME
           kubectl logs POD_NAME

Issue: Service not reachable
  Solution: kubectl get endpoints SERVICE_NAME
           kubectl port-forward svc/SERVICE_NAME PORT:PORT

Issue: Metrics not appearing in Prometheus
  Solution: Check pod logs for errors
           Verify scrape annotations on pods
           Check prometheus.yml scrape config

Issue: Docker build fails
  Solution: Ensure Docker is running
           Check Dockerfile syntax
           Verify docker build context path

8.2 MONITORING THE DEPLOYMENT
----------------------------
  kubectl get deploy: Show deployment status
  kubectl get pods: Show pod status and restart count
  kubectl get svc: Show service endpoints
  kubectl describe node: Node resource usage
  kubectl top pod: Pod CPU/memory usage (requires metrics-server)
  kubectl logs POD_NAME: Pod logs
  kubectl exec -it POD_NAME -- /bin/bash: Shell into pod

8.3 COMMON MAINTENANCE TASKS
---------------------------

Scale deployment:
  kubectl scale deploy/backend --replicas=5

Update image manually:
  kubectl set image deploy/backend backend=NEW_IMAGE:TAG

Restart pods:
  kubectl rollout restart deploy/backend

View rollout history:
  kubectl rollout history deploy/backend

Rollback to previous version:
  kubectl rollout undo deploy/backend

View resource usage:
  kubectl top nodes
  kubectl top pods

================================================================================
9. TECHNOLOGY STACK SUMMARY
================================================================================

Language & Frameworks:
  - Python 3.11 (backend)
  - FastAPI 0.104.1 (REST API framework)
  - Nginx (reverse proxy / web server)
  - HTML5/JavaScript (frontend static files)

Containerization:
  - Docker: Container images
  - Docker Compose: Local multi-container orchestration

Orchestration:
  - Kubernetes: Container orchestration platform
  - Helm: Kubernetes package manager (future use)

Infrastructure-as-Code:
  - Terraform: Declarative infrastructure provisioning
  - Kubernetes provider: Manage k8s resources

Automation:
  - Ansible: Build, push, deploy orchestration
  - Jenkins: CI/CD pipeline orchestration
  - Groovy: Jenkinsfile DSL

Monitoring & Observability:
  - Prometheus: Metrics collection and storage
  - Grafana: Visualization and dashboarding
  - prometheus-client: Python metrics library

Version Control:
  - Git: Source code management

Registries & Credentials:
  - Docker Hub: Container image registry (default)
  - Docker: Container runtime

Versions:
  - Kubernetes: 1.16+ (tested on minikube, EKS, AKS)
  - Docker: 20.10+
  - Terraform: 1.0.0+
  - Python: 3.11+
  - Nginx: latest (Alpine base)

================================================================================
10. QUICK START GUIDE
================================================================================

10.1 LOCAL DEVELOPMENT (5 minutes)
--------------------------------
1. Clone repository: git clone <repo-url>
2. Start services: docker-compose up -d
3. Wait for health checks: docker-compose ps
4. Access frontend: http://localhost
5. Test backend: curl http://localhost:8000/health
6. View logs: docker-compose logs -f

10.2 KUBERNETES DEPLOYMENT (15 minutes)
-------------------------------------
1. Ensure kubectl is configured: kubectl cluster-info
2. Initialize Terraform: terraform init
3. Plan deployment: terraform plan
4. Apply configuration: terraform apply
5. Verify deployments: kubectl get deploy,svc,pods
6. Port-forward to frontend: kubectl port-forward svc/frontend 8080:80
7. Access: http://localhost:8080

10.3 FULL CI/CD PIPELINE (30 minutes)
-----------------------------------
1. Clone repository
2. Install Jenkins and configure docker-creds credential
3. Create Jenkins pipeline job pointing to this repo
4. Configure Kubernetes cluster (kubeconfig on Jenkins agent)
5. Trigger build: Click "Build Now"
6. Monitor: Watch Jenkins UI for pipeline progress
7. After success: Services are deployed to Kubernetes

================================================================================
11. FILE STATISTICS & METRICS
================================================================================

Total project files: ~40 files

Largest files:
  - terraform/deployments.tf: 286 lines (Kubernetes Deployments)
  - terraform/services.tf: 190 lines (Kubernetes Services)
  - MONITORING.md: 271 lines (Prometheus/Grafana guide)
  - Jenkinsfile: 156 lines (CI/CD pipeline)

Smallest files:
  - main.tf: 8 lines (root module reference)
  - k8s/prometheus.rules: <5 lines (empty alerting rules)

Source code by language:
  - HCL (Terraform): ~1,500 lines
  - Python (FastAPI): 56 lines
  - YAML (K8s/Ansible): ~500 lines
  - Groovy (Jenkins): 156 lines
  - Shell (Dockerfiles): ~50 lines
  - Markdown (Documentation): ~800 lines
  - Configuration: nginx.conf, docker-compose.yml, etc.

Docker images produced:
  - devop-simple-project-backend: Python 3.11 + FastAPI
  - devop-simple-project-frontend: Nginx Alpine + HTML

Container sizes (typical):
  - Backend: ~200-300 MB
  - Frontend: ~50-100 MB

================================================================================
12. FUTURE ENHANCEMENTS & ROADMAP
================================================================================

Short-term (1-2 sprints):
  - Add unit tests for backend API
  - Add E2E tests for frontend
  - Configure Prometheus alerting rules
  - Create custom Grafana dashboards
  - Add database (PostgreSQL) for data persistence

Medium-term (2-4 sprints):
  - Implement Helm charts for easier management
  - Add Ingress with TLS
  - Implement GitOps (ArgoCD) for deployments
  - Add image scanning in CI/CD pipeline
  - Implement request tracing (Jaeger)

Long-term (4+ sprints):
  - Service mesh (Istio) for advanced traffic management
  - Multi-region/multi-cloud deployment
  - Disaster recovery procedures
  - Cost optimization and resource right-sizing
  - Security hardening and compliance scanning

================================================================================
END OF DOCUMENTATION
================================================================================

Document generation date: 2026-02-11
Last updated: [Now]
Total content: ~3,500 words

For questions or updates, refer to README.md, MONITORING.md, or TERRAFORM.md
in the project root directory.

================================================================================
